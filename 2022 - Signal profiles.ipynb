{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Signal profiles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# sns.set_style(\"whitegrid\")\n",
    "sns.set_style(\"white\")\n",
    "import os\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:21:11.784781Z",
     "end_time": "2023-05-11T14:21:11.787745Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GSE26320 reprocessed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GSE26320_PATH = os.path.expanduser('~/data/2023_GSE26320')\n",
    "GSE26320_MODIFICATIONS = ['CTCF', 'H3K27ac', 'H3K27me3', 'H3K36me3', 'H3K4me1', 'H3K4me3']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:21:11.787455Z",
     "end_time": "2023-05-11T14:21:11.821389Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Peaks length for ENCODE dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autocorrection across all coverage - with control correction\n",
    "Paper: Retrieving Chromatin Patterns from Deep Sequencing Data Using Correlation Functions\n",
    "https://www.cell.com/biophysj/fulltext/S0006-3495(17)30032-2#sec2\n",
    "\n",
    "`bwext` folder is used to create bigwig visualization by enlarging coverage to fragment size.\n",
    "```\n",
    "# Visualization\n",
    "for F in *.bam; do echo $F; samtools index $F; bamCoverage --ignoreDuplicates --extendReads 200 -b $F -p 6 -o ${F/.bam/_ext.bw}; done\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CHROM_SIZES = {\n",
    "    c: s for _, (c, s) in pd.read_csv(os.path.join(GSE26320_PATH, 'hg38.chrom.sizes'),\n",
    "                                      sep='\\t', names=['chr', 'size']).iterrows()\n",
    "}\n",
    "CHROM_SIZES"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:21:11.821326Z",
     "end_time": "2023-05-11T14:21:11.855262Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import fabs\n",
    "\n",
    "GSE26320_BW_PATH = os.path.join(GSE26320_PATH, 'bwext')\n",
    "\n",
    "def r0(o, c):\n",
    "    \"\"\" Correlation to estimate b \"\"\"\n",
    "    assert o.size == c.size\n",
    "    # o_mean = o.sum() / o.size\n",
    "    # c_mean = c.sum() / c.size\n",
    "    # on = o - o_mean\n",
    "    # cn = c - c_mean\n",
    "    # return np.dot(on, cn) / sqrt(np.dot(on, on) * np.dot(cn, cn))\n",
    "    return pearsonr(o, c)[0]\n",
    "\n",
    "def rDx(dx, o1, o2):\n",
    "    \"\"\" Slow shifted correlation function by formula from paper \"\"\"\n",
    "    assert o1.size == o2.size\n",
    "    # n = o1.size\n",
    "    # o1_mean = o1.sum() / o1.size\n",
    "    # o2_mean = o2.sum() / o2.size\n",
    "    # o1n = o1 - o1_mean\n",
    "    # o2n = o2 - o2_mean\n",
    "    # nominator = (1 / (2 * (n - dx))) * sum(\n",
    "    #     (o1[i] - o1_mean) * (o2[i + dx] - o2_mean) + (o1[i + dx] - o1_mean) * (o2[i] - o2_mean)\n",
    "    #     for i in range(n - dx)\n",
    "    # )\n",
    "    # denominator = (1 / n) * sqrt(np.dot(o1n, o1n) * np.dot(o2n, o2n))\n",
    "    # return nominator / denominator\n",
    "    return pearsonr(o1, np.roll(o2, dx))[0]\n",
    "\n",
    "# TODO: support blacklisted regions to avoid long-distance zero coverage\n",
    "\n",
    "def compute_autocorrelations_paper(modifications, cells, replicates, chrom_sizes, bin, ds, control='Input', bwpath=GSE26320_BW_PATH):\n",
    "    correlations = []\n",
    "    for mod, cell, rep in product(modifications, cells, replicates):\n",
    "        bwfile = next((f for f in os.listdir(bwpath) if '.bw' in f and mod in f and cell in f and rep in f), None)\n",
    "        bwcfile = next((f for f in os.listdir(bwpath) if '.bw' in f and control in f and cell in f and rep in f), None)\n",
    "        if bwfile is None or bwcfile is None:\n",
    "            continue\n",
    "        print(mod, cell, rep, bwfile, bwcfile)\n",
    "        for chr, size in tqdm(chrom_sizes.items()):\n",
    "            with pyBigWig.open(os.path.join(bwpath, bwfile)) as bw:\n",
    "                coverage = np.asarray(bw.stats(chr, nBins=(int(ceil(size / bin))), exact=True, type='sum'))\n",
    "            with pyBigWig.open(os.path.join(bwpath, bwcfile)) as bwc:\n",
    "                coveragec = np.asarray(bwc.stats(chr, nBins=(int(ceil(size / bin))), exact=True, type='sum'))\n",
    "            size = min(size, 50_000_000)\n",
    "            print(f'Computing {mod} {cell} {rep} binned {bin}bp coverage correlations on chr {chr} {size}')\n",
    "            # The coverage was initially calculated for each chromosome by extending the reads to\n",
    "            # fragment length, yielding a histogram with the genomic coordinate on the x axis\n",
    "            # and the number of reads per basepair on the y axis.\n",
    "\n",
    "            print(f'{chr} Coverage mln signal: {coverage.sum()} control: {coveragec.sum()}')\n",
    "            # First, the normalized coverage of the control Cnorm and of the specific IP Anorm\n",
    "            # were obtained by dividing by input signal I. Positions with zero input coverage were neglected.\n",
    "            # Subsequently, the coverage at these positions was set to the respective average value\n",
    "            # that was calculated for the remaining positions, thus eliminating fluctuations and\n",
    "            # corresponding contributions to the correlation coefficient from these positions.\n",
    "            coverage_mean = coverage.sum() / sum(coverage > 0)\n",
    "            coveragec_mean = coveragec.sum() / sum(coveragec > 0)\n",
    "            print(f'Mean signal {coverage_mean} control {coveragec_mean}')\n",
    "            coverage[coverage == 0] = coverage_mean\n",
    "            coveragec[coveragec == 0] = coveragec_mean\n",
    "            coverage = coverage / coverage_mean\n",
    "            coveragec = coveragec / coveragec_mean\n",
    "            # In the next step, nonspecific background signal was removed to obtain the\n",
    "            # normalized read occupancy O = Anorm − b × Cnorm\n",
    "            # The parameter b quantifies the contribution of the control signal present as\n",
    "            # background in the sample (IP).\n",
    "            # To estimate b, we minimized the absolute value of the Pearson correlation coefficient r0 at\n",
    "            # zero shift distance between the normalized occupancy O and the control coverage Cnorm.\n",
    "            # For the minimization procedure, b was changed between 0 and 1.\n",
    "            print('Estimating b between 0 and 1')\n",
    "            stepB = 0.001\n",
    "            bs = np.linspace(0, 1, num = int(1 / stepB))\n",
    "            bCs = [fabs(r0(coverage - b * coveragec, coveragec)) for b in bs]\n",
    "            bI = np.argmin(bCs)\n",
    "            b = bs[bI]\n",
    "            print(f'I={bI}, b={b}, correlation={bCs[bI]}')\n",
    "            coverageo = coverage - b * coveragec\n",
    "            print('Computing autocorrelations')\n",
    "            for d in ds:\n",
    "                corr, pval = rDx(d, coverageo, coverageo), None\n",
    "                correlations.append((mod, cell, rep, bwfile, chr, coverage_mean, coveragec_mean, b,\n",
    "                                     d * bin, corr, pval))\n",
    "    return pd.DataFrame(\n",
    "        columns=['modification', 'cell', 'replicate', 'file', 'chr', 'coverage mean', 'control mean', 'b',\n",
    "                 'd', 'correlation', 'pvalue'],\n",
    "        data=correlations\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T22:47:16.443247Z",
     "end_time": "2023-05-11T22:47:16.468663Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To sample the correlation function in a quasi-logarithmic manner (33),\n",
    "# profiles were binned by a factor of two after 25 shift operations to double the step size.\n",
    "# To preserve high resolution for small shift distances, the first binning operation was carried\n",
    "# out at a shift of Δx = 50 bp.\n",
    "BIN = 50\n",
    "DS = list(range(0, int(5000 / BIN))) + \\\n",
    "     list(range(int(5000 / BIN), int(100000 / BIN), int(100 / BIN))) + \\\n",
    "     list(range(int(10000 / BIN), int(1000000 / BIN), int(10000 / BIN)))\n",
    "print(len(DS))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T22:46:17.980868Z",
     "end_time": "2023-05-11T22:46:18.020716Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chr_sizes = {'chr22': CHROM_SIZES['chr22']}\n",
    "df_correlations = compute_autocorrelations_paper(GSE26320_MODIFICATIONS, ['K562'], ['rep1'], chr_sizes, BIN, DS)\n",
    "df_correlations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:21:11.887688Z",
     "end_time": "2023-05-11T14:35:10.583230Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "# Show aggregated data, since we don't want error plots\n",
    "t = df_correlations.copy()\n",
    "t.loc[t['d'] == 0, 'd'] = BIN / 2\n",
    "t = t[['modification', 'd', 'correlation']].groupby(['modification', 'd']).mean().reset_index()\n",
    "g_results = sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\",\n",
    "                         hue_order=['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3'])\n",
    "# sample_count = [1, 10, 50, 100, 200, 500, 1000, 2000, 5000,\n",
    "#                 10_000, 20_000, 50_000, 100_000, 200_000, 500_000, 1_000_000]\n",
    "sample_count = [100, 1000, 10_000, 100_000, 1_000_000]\n",
    "\n",
    "g_results.set(xscale='log')\n",
    "g_results.set(xticks=sample_count)\n",
    "g_results.set(xticklabels=sample_count)\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(BIN / 2, 1_000_000)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{GSE26320_PATH}/pics/paper_signal_correlation.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:35:10.580003Z",
     "end_time": "2023-05-11T14:35:11.097022Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\",\n",
    "             hue_order=['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3'])\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(0, 5000)\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{GSE26320_PATH}/pics/paper_signal_correlation_5000.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:35:11.094390Z",
     "end_time": "2023-05-11T14:35:11.547266Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Signal profile along peaks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RANGE = 50_000\n",
    "BIN = 50\n",
    "BINS = int(RANGE / BIN)\n",
    "PEAKS = 1000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:35:11.539149Z",
     "end_time": "2023-05-11T14:35:11.547340Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "GSE26320_SPAN_PATH = os.path.join(GSE26320_PATH, 'span')\n",
    "GSE26320_BW_PATH = os.path.join(GSE26320_PATH, 'bwext')\n",
    "\n",
    "RANGE = 10000\n",
    "BIN = 50\n",
    "BINS = int(RANGE / BIN)\n",
    "PEAKS = 100\n",
    "\n",
    "def intersect(c1, s1, e1, c2, s2, e2):\n",
    "    return c1 == c2 and (s1 <= s2 < e1 or s1 <= e2 - 1 < e2)\n",
    "\n",
    "def signal_peaks_profile(modifications, cells, replicates, peakspath=GSE26320_SPAN_PATH, bwpath=GSE26320_BW_PATH,\n",
    "                         control='Input'):\n",
    "    profiles = []\n",
    "    for mod, cell, rep in tqdm(list(product(modifications, cells, replicates))):\n",
    "        peaksfile = next((f for f in os.listdir(peakspath) if mod in f and cell in f and rep in f), None)\n",
    "        bwfile = next((f for f in os.listdir(bwpath) if '.bw' in f and mod in f and cell in f and rep in f), None)\n",
    "        bwcfile = next((f for f in os.listdir(bwpath) if '.bw' in f and control in f and cell in f and rep in f), None)\n",
    "        if peaksfile is None or bwfile is None:\n",
    "            continue\n",
    "        print(mod, cell, rep, peaksfile, bwfile)\n",
    "        # Load peaks file and sort by score\n",
    "        peaksfile_df = pd.read_csv(os.path.join(peakspath, peaksfile), sep='\\t',\n",
    "                                   names=['chr', 'start', 'end', '4', '5', '6', '7', '8', 'score'])\n",
    "        peaksfile_df = peaksfile_df[['chr', 'start', 'end', 'score']].copy()\n",
    "        peaksfile_df.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "        # Ensure that peaks do not intersect and doesn't contain any significant peaks\n",
    "        intersects = [t for _, t in peaksfile_df.head(PEAKS)[['chr', 'start', 'end']].iterrows()]\n",
    "        added = 0\n",
    "        with pyBigWig.open(os.path.join(bwpath, bwfile)) as bw:\n",
    "            with pyBigWig.open(os.path.join(bwpath, bwcfile)) as bwc:\n",
    "                total_coverage = sum(bw.stats(f'chr{i}', exact=True)[0] for i in range(1, 22))\n",
    "                total_coveragec = sum(bwc.stats(f'chr{i}', exact=True)[0] for i in range(1, 22))\n",
    "                # print(total_coverage)\n",
    "                for _, (chr, start, end, score) in peaksfile_df.iterrows():\n",
    "                    # No more than single intersection\n",
    "                    if sum(intersect(chr, start, end, c, s, e) for c, s, e in intersects) > 1:\n",
    "                        continue\n",
    "                    # Find summit for narrow peak, center of peak for broad\n",
    "                    if mod in ['CTCF', 'H3K27ac', 'H3K4me3']:\n",
    "                        nBins = min(BINS, int((end - start) / 20))\n",
    "                        stats = bw.stats(chr, start, end, nBins=nBins, exact=True)\n",
    "                        max_offset = start + (np.argmax(stats) + 0.5) * (end - start) / nBins\n",
    "                        r_start = int(max_offset  - RANGE / 2)\n",
    "                    else:\n",
    "                        r_start = int((start + end) / 2 - RANGE / 2)\n",
    "                    if r_start < 0:\n",
    "                        continue\n",
    "                    r_end = r_start + RANGE\n",
    "                    # Check that no peak intersect others\n",
    "                    if sum(intersect(chr, r_start, r_end, c, s, e) for c, s, e in intersects) > 1:\n",
    "                        continue\n",
    "                    intersects.append((chr, r_start, r_end))\n",
    "                    # Analyse signal and control coverage\n",
    "                    try:\n",
    "                        for i, cov in enumerate(bw.stats(chr, r_start, r_end, nBins=BINS, exact=True)):\n",
    "                            profiles.append((mod, cell, rep, added, (-BINS / 2 + i) * BIN, cov / total_coverage))\n",
    "                        for i, cov in enumerate(bwc.stats(chr, r_start, r_end, nBins=BINS, exact=True)):\n",
    "                            profiles.append((control, cell, rep, added, (-BINS / 2 + i) * BIN, cov / total_coveragec))\n",
    "                    except:\n",
    "                        pass\n",
    "                    added += 1\n",
    "                    if added == PEAKS:\n",
    "                        break\n",
    "\n",
    "    return pd.DataFrame(columns=['modification', 'cell', 'replicate', 'peak', 'x', 'coverage'], data=profiles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T23:10:11.903221Z",
     "end_time": "2023-05-11T23:10:11.928060Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_gse26320_cov = signal_peaks_profile(GSE26320_MODIFICATIONS, ['K562'], ['rep1'])\n",
    "df_gse26320_cov"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:35:11.539192Z",
     "end_time": "2023-05-11T14:35:35.045913Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(5, 3))\n",
    "# Show aggregated data, since we don't want error plots\n",
    "t = df_gse26320_cov[['modification', 'x', 'coverage']].groupby(['modification', 'x']).mean().reset_index()\n",
    "sns.lineplot(data=t, x=\"x\", y=\"coverage\", hue=\"modification\", err_style=None,\n",
    "             hue_order=['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{GSE26320_PATH}/pics/peaks_profile.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:35:35.052659Z",
     "end_time": "2023-05-11T14:35:35.386928Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Peaks densities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import os\n",
    "import pyBigWig\n",
    "\n",
    "GSE26320_SPAN_PATH = os.path.join(GSE26320_PATH, 'span')\n",
    "GSE26320_BW_PATH = os.path.join(GSE26320_PATH, 'bwext')\n",
    "\n",
    "data = []\n",
    "for mod, cell, rep in tqdm(list(product(GSE26320_MODIFICATIONS, ['K562'], ['rep1']))):\n",
    "    peaksfile = next((f for f in os.listdir(GSE26320_SPAN_PATH) if mod in f and cell in f and rep in f), None)\n",
    "    bwfile = next((f for f in os.listdir(GSE26320_BW_PATH) if '.bw' in f and mod in f and cell in f and rep in f), None)\n",
    "    bwcfile = next((f for f in os.listdir(GSE26320_BW_PATH) if '.bw' in f and 'Input' in f and cell in f and rep in f),\n",
    "                   None)\n",
    "    if peaksfile is None or bwfile is None or bwcfile is None:\n",
    "        continue\n",
    "    print(mod, cell, rep, peaksfile, bwfile, bwcfile)\n",
    "\n",
    "    # Load peaks file and sort by score\n",
    "    peaksfile_df = pd.read_csv(os.path.join(GSE26320_SPAN_PATH, peaksfile), sep='\\t',\n",
    "                               names=['chr', 'start', 'end', '4', '5', '6', '7', '8', 'score'])\n",
    "    with pyBigWig.open(os.path.join(GSE26320_BW_PATH, bwfile)) as bw:\n",
    "        with pyBigWig.open(os.path.join(GSE26320_BW_PATH, bwcfile)) as bwc:\n",
    "            # Resort top scored peaks by density and start with the most dense ones\n",
    "            total_coverage = sum(bw.stats(chr, exact=True, type='sum')[0] for chr in CHROM_SIZES.keys()) / 1e6\n",
    "            total_coveragec = sum(bwc.stats(chr, exact=True, type='sum')[0] for chr in CHROM_SIZES.keys()) / 1e6\n",
    "            for _, (chr, start, end) in peaksfile_df[['chr', 'start', 'end']].iterrows():\n",
    "                peak_coverage = bw.stats(chr, start, end, exact=True, type='sum')[0]\n",
    "                peak_coveragec = bwc.stats(chr, start, end, exact=True, type='sum')[0]\n",
    "                data.append((mod, cell, rep, total_coverage, total_coveragec,\n",
    "                             f'{chr}:{start}-{end}', end - start, peak_coverage, peak_coveragec))\n",
    "\n",
    "gse26320_peak_coverages_df = pd.DataFrame(\n",
    "    columns=['modification', 'cell', 'replicate', 'total_signal_coverage', 'total_control_coverage',\n",
    "             'peak', 'length', 'signal_coverage', 'control_coverage'],\n",
    "    data=data\n",
    ")\n",
    "del data\n",
    "gse26320_peak_coverages_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:35:35.386890Z",
     "end_time": "2023-05-11T14:36:12.357130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "t = pd.concat([gse26320_peak_coverages_df[gse26320_peak_coverages_df['modification'] == m].sample(2_000)\n",
    "               for m in gse26320_peak_coverages_df['modification'].unique()]).reset_index(drop=True)\n",
    "t['RPKM'] = t['signal_coverage'] / t['length'] * 1e3\n",
    "t['RPKM'] /= t['total_signal_coverage']\n",
    "t['RPKM'].clip(lower=0.2, inplace=True)\n",
    "t['length'].clip(upper=3e5, inplace=True)\n",
    "sample_count = [100,  1000, 10_000, 100_000]\n",
    "g_results = sns.scatterplot(data=t, x='length', y='RPKM', hue='modification', alpha=0.3,\n",
    "                            hue_order=['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3'])\n",
    "g_results.set(xscale='log')\n",
    "g_results.set(xticks=sample_count)\n",
    "g_results.set(xticklabels=sample_count)\n",
    "g_results.set(yscale='log')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{GSE26320_PATH}/pics/peaks_length_vs_rpkm.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:36:12.361659Z",
     "end_time": "2023-05-11T14:36:14.478188Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autocorrelations - without control correction\n",
    "Simplified autocorrelation computation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def compute_autocorrelations(modifications, cells, replicates, chrom_sizes, bin, max_d, bwpath=GSE26320_BW_PATH):\n",
    "    correlations = []\n",
    "    for mod, cell, rep in tqdm(list(product(modifications, cells, replicates))):\n",
    "        bwfile = next((f for f in os.listdir(bwpath) if '.bw' in f and mod in f and cell in f and rep in f), None)\n",
    "        if bwfile is None:\n",
    "            continue\n",
    "        print(mod, cell, rep, bwfile, bwcfile)\n",
    "        print(f'Computing binned coverage correlations')\n",
    "        with pyBigWig.open(os.path.join(bwpath, bwfile)) as bw:\n",
    "            total_coverage = sum(bw.stats(chr, exact=True, type='sum')[0] for chr in CHROM_SIZES.keys()) / 1e6\n",
    "            print(f'Signal coverage mln {total_coverage} control {total_coveragec}')\n",
    "            for chr, size in tqdm(chrom_sizes.items()):\n",
    "                # TODO: support blacklisted regions to avoid long-distance zero coverage\n",
    "                coverage = np.asarray(bw.stats(chr, 0, size, nBins=(int(ceil((size) / bin))),\n",
    "                                               exact=True, type='sum'))\n",
    "                # Ignore non-covered regions\n",
    "                coverage = coverage[coverage > 0]\n",
    "                for d in range(0, max_d):\n",
    "                    corr, pval = pearsonr(coverage, np.roll(coverage, d))\n",
    "                    correlations.append((mod, cell, rep, bwfile, chr, d * bin, corr, pval))\n",
    "    return pd.DataFrame(\n",
    "        columns=['modification', 'cell', 'replicate', 'file', 'chr', 'd', 'correlation', 'pvalue'],\n",
    "        data=correlations\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:36:14.476709Z",
     "end_time": "2023-05-11T14:36:14.478705Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_correlations = compute_autocorrelations(['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3', 'Input'],\n",
    "                                           ['K562'], ['rep1'], {'chr22': CHROM_SIZES['chr22']}, BIN, 5000)\n",
    "df_correlations"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:36:14.476801Z",
     "end_time": "2023-05-11T14:45:16.542747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "# Show aggregated data, since we don't want error plots\n",
    "t = df_correlations.copy()\n",
    "t.loc[t['d'] == 0, 'd'] = BIN - 1\n",
    "t = df_correlations[['modification', 'd', 'correlation']].groupby(['modification', 'd']).mean().reset_index()\n",
    "g_results = sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\",\n",
    "                         hue_order=['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3', 'Input'])\n",
    "sample_count = [100,  1000, 10_000, 100_000]\n",
    "g_results.set(xscale='log')\n",
    "g_results.set(xticks=sample_count)\n",
    "g_results.set(xticklabels=sample_count)\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(BIN - 1, 100_000)\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{GSE26320_PATH}/pics/signal_correlation.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:45:16.542582Z",
     "end_time": "2023-05-11T14:45:17.662715Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\",\n",
    "             hue_order=['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3', 'Input'])\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(0, 5000)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{GSE26320_PATH}/pics/signal_correlation_5000.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T14:45:17.662260Z",
     "end_time": "2023-05-11T14:45:18.412289Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RoadmapEpigenomics autocorrelation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PATH = os.path.expanduser('~/data/2023_Immune')\n",
    "\n",
    "IMMUNE_CELLS = ['BCell', 'TCell', 'Monocyte']\n",
    "MODIFICATIONS = ['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T22:46:25.344210Z",
     "end_time": "2023-05-11T22:46:25.379455Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chr_sizes = {'chr22': CHROM_SIZES['chr22']}\n",
    "df_correlations = compute_autocorrelations_paper(MODIFICATIONS, ['BCell'], [''], chr_sizes, BIN, DS, control='Control', bwpath=PATH + '/bwext')\n",
    "df_correlations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T22:49:53.618537Z",
     "end_time": "2023-05-11T23:03:48.303136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "# Show aggregated data, since we don't want error plots\n",
    "t = df_correlations.copy()\n",
    "t.loc[t['d'] == 0, 'd'] = BIN / 2\n",
    "t = t[['modification', 'd', 'correlation']].groupby(['modification', 'd']).mean().reset_index()\n",
    "g_results = sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\",\n",
    "                         hue_order=['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3'])\n",
    "# sample_count = [1, 10, 50, 100, 200, 500, 1000, 2000, 5000,\n",
    "#                 10_000, 20_000, 50_000, 100_000, 200_000, 500_000, 1_000_000]\n",
    "sample_count = [100, 1000, 10_000, 100_000, 1_000_000]\n",
    "\n",
    "g_results.set(xscale='log')\n",
    "g_results.set(xticks=sample_count)\n",
    "g_results.set(xticklabels=sample_count)\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(BIN / 2, 1_000_000)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PATH}/pics/paper_signal_correlation.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T23:03:48.320837Z",
     "end_time": "2023-05-11T23:03:49.165890Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Peaks profile"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMMUNE_SPAN_PATH = os.path.join(PATH, 'span')\n",
    "IMMUNE_BW_PATH = os.path.join(PATH, 'bwext')\n",
    "\n",
    "df_immune_cov = signal_peaks_profile(MODIFICATIONS, ['BCell'], [''], IMMUNE_SPAN_PATH, IMMUNE_BW_PATH, 'Control')\n",
    "df_immune_cov"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T23:11:55.265573Z",
     "end_time": "2023-05-11T23:12:21.098203Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "# Show aggregated data, since we don't want error plots\n",
    "t = df_gse26320_cov[['modification', 'x', 'coverage']].groupby(['modification', 'x']).mean().reset_index()\n",
    "sns.lineplot(data=t, x=\"x\", y=\"coverage\", hue=\"modification\", err_style=None,\n",
    "             hue_order=['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PATH}/pics/peaks_profile.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T23:12:25.708472Z",
     "end_time": "2023-05-11T23:12:26.047038Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Peaks intensities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = []\n",
    "for mod, cell, rep in tqdm(list(product(MODIFICATIONS, ['BCell'], ['']))):\n",
    "    peaksfile = next((f for f in os.listdir(IMMUNE_SPAN_PATH) if mod in f and cell in f and rep in f), None)\n",
    "    bwfile = next((f for f in os.listdir(IMMUNE_BW_PATH) if '.bw' in f and mod in f and cell in f and rep in f), None)\n",
    "    bwcfile = next((f for f in os.listdir(IMMUNE_BW_PATH) if '.bw' in f and 'Control' in f and cell in f and rep in f), None)\n",
    "    if peaksfile is None or bwfile is None or bwcfile is None:\n",
    "        continue\n",
    "    print(mod, cell, rep, peaksfile, bwfile, bwcfile)\n",
    "\n",
    "    # Load peaks file and sort by score\n",
    "    peaksfile_df = pd.read_csv(os.path.join(IMMUNE_SPAN_PATH, peaksfile), sep='\\t',\n",
    "                               names=['chr', 'start', 'end', '4', '5', '6', '7', '8', 'score'])\n",
    "    with pyBigWig.open(os.path.join(IMMUNE_BW_PATH, bwfile)) as bw:\n",
    "        with pyBigWig.open(os.path.join(IMMUNE_BW_PATH, bwcfile)) as bwc:\n",
    "            # Resort top scored peaks by density and start with the most dense ones\n",
    "            total_coverage = sum(bw.stats(chr, exact=True, type='sum')[0] for chr in CHROM_SIZES.keys() if '_' not in chr) / 1e6\n",
    "            total_coveragec = sum(bwc.stats(chr, exact=True, type='sum')[0] for chr in CHROM_SIZES.keys() if '_' not in chr) / 1e6\n",
    "            for _, (chr, start, end) in peaksfile_df[['chr', 'start', 'end']].iterrows():\n",
    "                if '_' in chr:\n",
    "                    continue\n",
    "                peak_coverage = bw.stats(chr, start, end, exact=True, type='sum')[0]\n",
    "                peak_coveragec = bwc.stats(chr, start, end, exact=True, type='sum')[0]\n",
    "                data.append((mod, cell, rep, total_coverage, total_coveragec,\n",
    "                             f'{chr}:{start}-{end}', end - start, peak_coverage, peak_coveragec))\n",
    "\n",
    "immune_peak_coverages_df = pd.DataFrame(\n",
    "    columns=['modification', 'cell', 'replicate', 'total_signal_coverage', 'total_control_coverage',\n",
    "             'peak', 'length', 'signal_coverage', 'control_coverage'],\n",
    "    data=data\n",
    ")\n",
    "del data\n",
    "immune_peak_coverages_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T23:07:43.173811Z",
     "end_time": "2023-05-11T23:08:26.060034Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "t = pd.concat([immune_peak_coverages_df[immune_peak_coverages_df['modification'] == m].sample(2_000)\n",
    "               for m in immune_peak_coverages_df['modification'].unique()]).reset_index(drop=True)\n",
    "t['RPKM'] = t['signal_coverage'] / t['length'] * 1e3\n",
    "t['RPKM'] /= t['total_signal_coverage']\n",
    "t['RPKM'].clip(lower=0.2, inplace=True)\n",
    "t['length'].clip(upper=3e5, inplace=True)\n",
    "sample_count = [100,  1000, 10_000, 100_000]\n",
    "g_results = sns.scatterplot(data=t, x='length', y='RPKM', hue='modification', alpha=0.3,\n",
    "                            hue_order=['H3K27ac', 'H3K4me3', 'H3K4me1', 'H3K36me3'])\n",
    "g_results.set(xscale='log')\n",
    "g_results.set(xticks=sample_count)\n",
    "g_results.set(xticklabels=sample_count)\n",
    "g_results.set(yscale='log')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PATH}/pics/peaks_length_vs_rpkm.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T23:08:26.059852Z",
     "end_time": "2023-05-11T23:08:28.151701Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
