{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Signal profiles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# % matplotlib inline\n",
    "# % config InlineBackend.figure_format='retina'\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GSE26320_RAW reprocessed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Don't have H1 H3K27ac rep2\n",
    "# Don't have Huvec H3K4me3 rep1\n",
    "# Don't have HepG2 H3K4me1 rep2\n",
    "GSE26320_PATH = os.path.expanduser('~/data/GSE26320_RAW')\n",
    "GSE26320_CELLS = ['GM12878', 'HMEC', 'HSMM', 'K562', 'NHEK', 'NHLF', 'H1', 'Huvec', 'HepG2']\n",
    "GSE26320_MODIFICATIONS = ['CTCF', 'H3K27ac', 'H3K27me3', 'H3K36me3', 'H3K4me1', 'H3K4me3']\n",
    "GSE26320_REPS = ['rep1', 'rep2']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autocorrelation across all coverage\n",
    "Paper: Retrieving Chromatin Patterns from Deep Sequencing Data Using Correlation Functions\n",
    "https://www.cell.com/biophysj/fulltext/S0006-3495(17)30032-2#sec2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CHROM_SIZES = {\n",
    "    c: s for _, (c, s) in pd.read_csv(os.path.join(GSE26320_PATH, 'hg19.chrom.sizes'),\n",
    "                                      sep='\\t', names=['chr', 'size']).iterrows()\n",
    "}\n",
    "CHROM_SIZES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from math import fabs, sqrt\n",
    "import pyBigWig\n",
    "\n",
    "\n",
    "GSE26320_BW_PATH = os.path.join(GSE26320_PATH, 'bwext')\n",
    "\n",
    "def r0(o, c):\n",
    "    \"\"\" Correlation to estimate b \"\"\"\n",
    "    assert o.size == c.size\n",
    "    # o_mean = o.sum() / o.size\n",
    "    # c_mean = c.sum() / c.size\n",
    "    # on = o - o_mean\n",
    "    # cn = c - c_mean\n",
    "    # return np.dot(on, cn) / sqrt(np.dot(on, on) * np.dot(cn, cn))\n",
    "    return pearsonr(o, c)[0]\n",
    "\n",
    "def rDx(dx, o1, o2):\n",
    "    \"\"\" Slow shifted correlation function by formula from paper \"\"\"\n",
    "    assert o1.size == o2.size\n",
    "    # n = o1.size\n",
    "    # o1_mean = o1.sum() / o1.size\n",
    "    # o2_mean = o2.sum() / o2.size\n",
    "    # o1n = o1 - o1_mean\n",
    "    # o2n = o2 - o2_mean\n",
    "    # nominator = (1 / (2 * (n - dx))) * sum(\n",
    "    #     (o1[i] - o1_mean) * (o2[i + dx] - o2_mean) + (o1[i + dx] - o1_mean) * (o2[i] - o2_mean)\n",
    "    #     for i in range(n - dx)\n",
    "    # )\n",
    "    # denominator = (1 / n) * sqrt(np.dot(o1n, o1n) * np.dot(o2n, o2n))\n",
    "    # return nominator / denominator\n",
    "    return pearsonr(o1, np.roll(o2, dx))[0]\n",
    "\n",
    "# TODO: support blacklisted regions to avoid long-distance zero coverage\n",
    "\n",
    "def compute_autocorrelations_paper(modifications, cells, replicates, chrom_sizes, bin, ds):\n",
    "    correlations = []\n",
    "    for mod, cell, rep in tqdm(list(product(modifications, cells, replicates))):\n",
    "        bwfile = next((f for f in os.listdir(GSE26320_BW_PATH) if '.bw' in f and mod in f and cell in f and rep in f), None)\n",
    "        bwcfile = next((f for f in os.listdir(GSE26320_BW_PATH) if '.bw' in f and 'WCE' in f and cell in f and rep in f), None)\n",
    "        if bwfile is None or bwcfile is None:\n",
    "            continue\n",
    "        print(mod, cell, rep, bwfile, bwcfile)\n",
    "        for chr, size in chrom_sizes.items():\n",
    "            with pyBigWig.open(os.path.join(GSE26320_BW_PATH, bwfile)) as bw:\n",
    "                coverage = np.asarray(bw.stats(chr, nBins=(int(ceil(size / bin))), exact=True, type='sum'))\n",
    "            with pyBigWig.open(os.path.join(GSE26320_BW_PATH, bwcfile)) as bwc:\n",
    "                coveragec = np.asarray(bwc.stats(chr, nBins=(int(ceil(size / bin))), exact=True, type='sum'))\n",
    "            size = min(size, 50_000_000)\n",
    "            print(f'Computing {mod} {cell} {rep} binned {bin}bp coverage correlations on chr {chr} {size}')\n",
    "            # The coverage was initially calculated for each chromosome by extending the reads to\n",
    "            # fragment length, yielding a histogram with the genomic coordinate on the x axis\n",
    "            # and the number of reads per basepair on the y axis.\n",
    "\n",
    "            print(f'{chr} Coverage mln signal: {coverage.sum()} control: {coveragec.sum()}')\n",
    "            # First, the normalized coverage of the control Cnorm and of the specific IP Anorm\n",
    "            # were obtained by dividing by input signal I. Positions with zero input coverage were neglected.\n",
    "            # Subsequently, the coverage at these positions was set to the respective average value\n",
    "            # that was calculated for the remaining positions, thus eliminating fluctuations and\n",
    "            # corresponding contributions to the correlation coefficient from these positions.\n",
    "            coverage_mean = coverage.sum() / sum(coverage > 0)\n",
    "            coveragec_mean = coveragec.sum() / sum(coveragec > 0)\n",
    "            print(f'Mean signal {coverage_mean} control {coveragec_mean}')\n",
    "            coverage[coverage == 0] = coverage_mean\n",
    "            coveragec[coveragec == 0] = coveragec_mean\n",
    "            coverage = coverage / coverage_mean\n",
    "            coveragec = coveragec / coveragec_mean\n",
    "            # In the next step, nonspecific background signal was removed to obtain the\n",
    "            # normalized read occupancy O = Anorm − b × Cnorm\n",
    "            # The parameter b quantifies the contribution of the control signal present as\n",
    "            # background in the sample (IP).\n",
    "            # To estimate b, we minimized the absolute value of the Pearson correlation coefficient r0 at\n",
    "            # zero shift distance between the normalized occupancy O and the control coverage Cnorm.\n",
    "            # For the minimization procedure, b was changed between 0 and 1.\n",
    "            print('Estimating b between 0 and 1')\n",
    "            stepB = 0.001\n",
    "            bs = np.linspace(0, 1, num = int(1 / stepB))\n",
    "            bCs = [fabs(r0(coverage - b * coveragec, coveragec)) for b in bs]\n",
    "            bI = np.argmin(bCs)\n",
    "            b = bs[bI]\n",
    "            print(f'I={bI}, b={b}, correlation={bCs[bI]}')\n",
    "            coverageo = coverage - b * coveragec\n",
    "            print('Computing autocorrelations')\n",
    "            for d in ds:\n",
    "                corr, pval = rDx(d, coverageo, coverageo), None\n",
    "                correlations.append((mod, cell, rep, bwfile, chr, coverage_mean, coveragec_mean, b,\n",
    "                                     d * bin, corr, pval))\n",
    "    return pd.DataFrame(\n",
    "        columns=['modification', 'cell', 'replicate', 'file', 'chr', 'coverage mean', 'control mean', 'b',\n",
    "                 'd', 'correlation', 'pvalue'],\n",
    "        data=correlations\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To sample the correlation function in a quasi-logarithmic manner (33),\n",
    "# profiles were binned by a factor of two after 25 shift operations to double the step size.\n",
    "# To preserve high resolution for small shift distances, the first binning operation was carried\n",
    "# out at a shift of Δx = 50 bp.\n",
    "BIN = 10\n",
    "DS = list(range(0, int(5000 / BIN))) + \\\n",
    "     list(range(int(5000 / BIN), int(100000 / BIN), int(100 / BIN))) + \\\n",
    "     list(range(int(10000 / BIN), int(1000000 / BIN), int(10000 / BIN)))\n",
    "print(len(DS))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chr_sizes = {'chr22': CHROM_SIZES['chr22']}\n",
    "df_correlations = compute_autocorrelations_paper(GSE26320_MODIFICATIONS, ['K562'], ['rep1'], chr_sizes, BIN, DS)\n",
    "df_correlations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.axes()\n",
    "# Show aggregated data, since we don't want error plots\n",
    "t = df_correlations.copy()\n",
    "t.loc[t['d'] == 0, 'd'] = BIN / 2\n",
    "t = t[['modification', 'd', 'correlation']].groupby(['modification', 'd']).mean().reset_index()\n",
    "g_results = sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\")\n",
    "sample_count = [1, 10, 50, 100, 200, 500, 1000, 2000, 5000,\n",
    "                10_000, 20_000, 50_000, 100_000, 200_000, 500_000, 1_000_000]\n",
    "g_results.set(xscale='log')\n",
    "g_results.set(xticks=sample_count)\n",
    "g_results.set(xticklabels=sample_count)\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(BIN / 2, 1_000_000)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{GSE26320_PATH}/analyze/signal_correlation.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.axes()\n",
    "sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\")\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(0, 5000)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot autocorrelation from published bedgraph\n",
    "\n",
    "Downloaded from https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE61874"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GSE61874_bedgraphs = os.path.expanduser('~/data/GSE61874_bedgraphs')\n",
    "\n",
    "def compute_autocorrelations_bedgraph(bin, ds):\n",
    "    correlations = []\n",
    "    for mod, file in [('H3K36me3', 'ESC_H3K36me3_1_norm_chr1.bedgraph'), ('H3K4me3', 'ESC_H3K4me3_1_norm_chr1.bedgraph')]:\n",
    "        print(mod, file)\n",
    "        df = pd.read_csv(os.path.join(GSE61874_bedgraphs, file), names=['chr', 'start', 'end', 'score'], header=None, sep='\\t')\n",
    "        display(df.head(1))\n",
    "        size = 50_000_000\n",
    "        print(f'Computing {mod} binned {bin}bp coverage correlations on chr1 {size}')\n",
    "        coverageo = np.zeros(int(ceil(size / bin)))\n",
    "        lastI = 0\n",
    "        lastAccum = 0\n",
    "        for _, (chr, start, end, score) in df.iterrows():\n",
    "            nextI = int(ceil(end / bin))\n",
    "            for i in range(lastI, nextI):\n",
    "                if i >= coverageo.size:\n",
    "                    break\n",
    "                coverageo[i] = bin * score + lastAccum\n",
    "                lastAccum = 0\n",
    "            lastAccum = (end - nextI * bin) * score\n",
    "            lastI = nextI\n",
    "            if lastI >= coverageo.size:\n",
    "                break\n",
    "        print(f'Coverage, {coverageo[:100]}')\n",
    "        print('Computing autocorrelations')\n",
    "        for d in ds:\n",
    "            corr, pval = pearsonr(coverageo, np.roll(coverageo, d))\n",
    "            # corr, pval = rDx(d, coverageo, coverageo), None\n",
    "            correlations.append((mod, d * bin, corr, pval))\n",
    "    return pd.DataFrame(\n",
    "        columns=['modification', 'd', 'correlation', 'pvalue'],\n",
    "        data=correlations\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BIN = 50\n",
    "DS = list(range(0, int(5000 / BIN))) + \\\n",
    "     list(range(int(5000 / BIN), int(100000 / BIN), int(100 / BIN))) + \\\n",
    "     list(range(int(10000 / BIN), int(1000000 / BIN), int(10000 / BIN)))\n",
    "print(len(DS))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_correlations_bedgraph =  compute_autocorrelations_bedgraph(BIN, DS)\n",
    "df_correlations_bedgraph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.axes()\n",
    "# Show aggregated data, since we don't want error plots\n",
    "t = df_correlations_bedgraph.copy()\n",
    "t.loc[t['d'] == 0, 'd'] = BIN / 2\n",
    "t = t[['modification', 'd', 'correlation']].groupby(['modification', 'd']).mean().reset_index()\n",
    "g_results = sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\")\n",
    "sample_count = [1, 10, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 500000, 1000000]\n",
    "g_results.set(xscale='log')\n",
    "g_results.set(xticks=sample_count)\n",
    "g_results.set(xticklabels=sample_count)\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(BIN / 2, 1000000)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.axes()\n",
    "sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\")\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(0, 5000)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Signal profile along peaks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RANGE = 50_000\n",
    "BIN = 50\n",
    "BINS = int(RANGE / BIN)\n",
    "PEAKS = 1000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import os\n",
    "import pyBigWig\n",
    "\n",
    "GSE26320_SPAN_PATH = os.path.join(GSE26320_PATH, 'span')\n",
    "GSE26320_BW_PATH = os.path.join(GSE26320_PATH, 'bwext')\n",
    "\n",
    "RANGE = 10000\n",
    "BIN = 50\n",
    "BINS = int(RANGE / BIN)\n",
    "PEAKS = 100\n",
    "\n",
    "def intersect(c1, s1, e1, c2, s2, e2):\n",
    "    return c1 == c2 and (s1 <= s2 < e1 or s1 <= e2 - 1 < e2)\n",
    "\n",
    "def signal_peaks_profile(modifications, cells, replicates):\n",
    "    profiles = []\n",
    "    for mod, cell, rep in tqdm(list(product(modifications, cells, replicates))):\n",
    "        peaksfile = next((f for f in os.listdir(GSE26320_SPAN_PATH) if mod in f and cell in f and rep in f), None)\n",
    "        bwfile = next((f for f in os.listdir(GSE26320_BW_PATH) if '.bw' in f and mod in f and cell in f and rep in f), None)\n",
    "        bwcfile = next((f for f in os.listdir(GSE26320_BW_PATH) if '.bw' in f and 'WCE' in f and cell in f and rep in f), None)\n",
    "        if peaksfile is None or bwfile is None:\n",
    "            continue\n",
    "        print(mod, cell, rep, peaksfile, bwfile)\n",
    "        # Load peaks file and sort by score\n",
    "        peaksfile_df = pd.read_csv(os.path.join(GSE26320_SPAN_PATH, peaksfile), sep='\\t',\n",
    "                                   names=['chr', 'start', 'end', '4', '5', '6', '7', '8', 'score'])\n",
    "        peaksfile_df = peaksfile_df[['chr', 'start', 'end', 'score']].copy()\n",
    "        peaksfile_df.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "        # Ensure that peaks do not intersect and doesn't contain any significant peaks\n",
    "        intersects = [t for _, t in peaksfile_df.head(PEAKS)[['chr', 'start', 'end']].iterrows()]\n",
    "        added = 0\n",
    "        with pyBigWig.open(os.path.join(GSE26320_BW_PATH, bwfile)) as bw:\n",
    "            with pyBigWig.open(os.path.join(GSE26320_BW_PATH, bwcfile)) as bwc:\n",
    "                total_coverage = sum(bw.stats(f'chr{i}', exact=True)[0] for i in range(1, 22))\n",
    "                total_coveragec = sum(bwc.stats(f'chr{i}', exact=True)[0] for i in range(1, 22))\n",
    "                # print(total_coverage)\n",
    "                for _, (chr, start, end, score) in peaksfile_df.iterrows():\n",
    "                    # No more than single intersection\n",
    "                    if sum(intersect(chr, start, end, c, s, e) for c, s, e in intersects) > 1:\n",
    "                        continue\n",
    "                    # Find summit for narrow peak, center of peak for broad\n",
    "                    if mod in ['CTCF', 'H3K27ac', 'H3K4me3']:\n",
    "                        nBins = min(BINS, int((end - start) / 20))\n",
    "                        stats = bw.stats(chr, start, end, nBins=nBins, exact=True)\n",
    "                        max_offset = start + (np.argmax(stats) + 0.5) * (end - start) / nBins\n",
    "                        r_start = int(max_offset  - RANGE / 2)\n",
    "                    else:\n",
    "                        r_start = int((start + end) / 2 - RANGE / 2)\n",
    "                    if r_start < 0:\n",
    "                        continue\n",
    "                    r_end = r_start + RANGE\n",
    "                    # Check that no peak intersect others\n",
    "                    if sum(intersect(chr, r_start, r_end, c, s, e) for c, s, e in intersects) > 1:\n",
    "                        continue\n",
    "                    intersects.append((chr, r_start, r_end))\n",
    "                    # Analyse signal and control coverage\n",
    "                    try:\n",
    "                        for i, cov in enumerate(bw.stats(chr, r_start, r_end, nBins=BINS, exact=True)):\n",
    "                            profiles.append((mod, cell, rep, added, (-BINS / 2 + i) * BIN, cov / total_coverage))\n",
    "                        for i, cov in enumerate(bwc.stats(chr, r_start, r_end, nBins=BINS, exact=True)):\n",
    "                            profiles.append(('Input', cell, rep, added, (-BINS / 2 + i) * BIN, cov / total_coveragec))\n",
    "                    except:\n",
    "                        pass\n",
    "                    added += 1\n",
    "                    if added == PEAKS:\n",
    "                        break\n",
    "\n",
    "    return pd.DataFrame(columns=['modification', 'cell', 'replicate', 'peak', 'x', 'coverage'], data=profiles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_gse26320_cov = signal_peaks_profile(GSE26320_MODIFICATIONS, ['K562'], ['rep1'])\n",
    "df_gse26320_cov"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 8))\n",
    "# Show aggregated data, since we don't want error plots\n",
    "t = df_gse26320_cov[['modification', 'x', 'coverage']].groupby(['modification', 'x']).mean().reset_index()\n",
    "sns.lineplot(data=t, x=\"x\", y=\"coverage\", hue=\"modification\", err_style=None)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{GSE26320_PATH}/analyze/peaks_profile.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Peaks densities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import os\n",
    "import pyBigWig\n",
    "\n",
    "GSE26320_SPAN_PATH = os.path.join(GSE26320_PATH, 'span')\n",
    "GSE26320_BW_PATH = os.path.join(GSE26320_PATH, 'bwext')\n",
    "\n",
    "data = []\n",
    "for mod, cell, rep in tqdm(list(product(GSE26320_MODIFICATIONS, ['K562'], ['rep1']))):\n",
    "    peaksfile = next((f for f in os.listdir(GSE26320_SPAN_PATH) if mod in f and cell in f and rep in f), None)\n",
    "    bwfile = next((f for f in os.listdir(GSE26320_BW_PATH) if '.bw' in f and mod in f and cell in f and rep in f), None)\n",
    "    bwcfile = next((f for f in os.listdir(GSE26320_BW_PATH) if '.bw' in f and 'WCE' in f and cell in f and rep in f),\n",
    "                   None)\n",
    "    if peaksfile is None or bwfile is None or bwcfile is None:\n",
    "        continue\n",
    "    print(mod, cell, rep, peaksfile, bwfile, bwcfile)\n",
    "\n",
    "    # Load peaks file and sort by score\n",
    "    peaksfile_df = pd.read_csv(os.path.join(GSE26320_SPAN_PATH, peaksfile), sep='\\t',\n",
    "                               names=['chr', 'start', 'end', '4', '5', '6', '7', '8', 'score'])\n",
    "    with pyBigWig.open(os.path.join(GSE26320_BW_PATH, bwfile)) as bw:\n",
    "        with pyBigWig.open(os.path.join(GSE26320_BW_PATH, bwcfile)) as bwc:\n",
    "            # Resort top scored peaks by density and start with the most dense ones\n",
    "            total_coverage = sum(bw.stats(chr, exact=True, type='sum')[0] for chr in CHROM_SIZES.keys()) / 1e6\n",
    "            total_coveragec = sum(bwc.stats(chr, exact=True, type='sum')[0] for chr in CHROM_SIZES.keys()) / 1e6\n",
    "            for _, (chr, start, end) in peaksfile_df[['chr', 'start', 'end']].iterrows():\n",
    "                peak_coverage = bw.stats(chr, start, end, exact=True, type='sum')[0]\n",
    "                peak_coveragec = bwc.stats(chr, start, end, exact=True, type='sum')[0]\n",
    "                data.append((mod, cell, rep, total_coverage, total_coveragec,\n",
    "                             f'{chr}:{start}-{end}', end - start, peak_coverage, peak_coveragec))\n",
    "\n",
    "peak_coverages_df = pd.DataFrame(\n",
    "    columns=['modification', 'cell', 'replicate', 'total_signal_coverage', 'total_control_coverage',\n",
    "             'peak', 'length', 'signal_coverage', 'control_coverage'],\n",
    "    data=data\n",
    ")\n",
    "del data\n",
    "peak_coverages_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "t = peak_coverages_df.copy()\n",
    "t['RPKM'] = t['signal_coverage'] / t['length'] * 1e3\n",
    "t['RPKM'] /= t['total_signal_coverage']\n",
    "t['RPKM'].clip(lower=0.2, inplace=True)\n",
    "t['length'].clip(upper=3e5, inplace=True)\n",
    "g_results = sns.scatterplot(data=t, x='length', y='RPKM', hue='modification', alpha=0.3)\n",
    "sample_count = [200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000]\n",
    "g_results.set(xscale='log')\n",
    "g_results.set(xticks=sample_count)\n",
    "g_results.set(xticklabels=sample_count)\n",
    "g_results.set(yscale='log')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{GSE26320_PATH}/analyze/peaks_length_vs_rpkm.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autocorrelations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def compute_autocorrelations(modifications, cells, replicates, chrom_sizes, bin, max_d):\n",
    "    correlations = []\n",
    "    for mod, cell, rep in tqdm(list(product(modifications, cells, replicates))):\n",
    "        bwfile = next((f for f in os.listdir(GSE26320_BW_PATH) if '.bw' in f and mod in f and cell in f and rep in f), None)\n",
    "        if bwfile is None:\n",
    "            continue\n",
    "        print(mod, cell, rep, bwfile, bwcfile)\n",
    "        print(f'Computing binned coverage correlations')\n",
    "        with pyBigWig.open(os.path.join(GSE26320_BW_PATH, bwfile)) as bw:\n",
    "            total_coverage = sum(bw.stats(chr, exact=True, type='sum')[0] for chr in CHROM_SIZES.keys()) / 1e6\n",
    "            print(f'Signal coverage mln {total_coverage} control {total_coveragec}')\n",
    "            for chr, size in tqdm(chrom_sizes.items()):\n",
    "                # TODO: support blacklisted regions to avoid long-distance zero coverage\n",
    "                coverage = np.asarray(bw.stats(chr, 0, size, nBins=(int(ceil((size) / bin))),\n",
    "                                               exact=True, type='sum'))\n",
    "                # Ignore non-covered regions\n",
    "                coverage = coverage[coverage > 0]\n",
    "                for d in range(0, max_d):\n",
    "                    corr, pval = pearsonr(coverage, np.roll(coverage, d))\n",
    "                    correlations.append((mod, cell, rep, bwfile, chr, d * bin, corr, pval))\n",
    "    return pd.DataFrame(\n",
    "        columns=['modification', 'cell', 'replicate', 'file', 'chr', 'd', 'correlation', 'pvalue'],\n",
    "        data=correlations\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_correlations = compute_autocorrelations(['H3K4me3', 'H3K36me3', 'WCE'], ['K562'], ['rep1'], {'chr22': CHROM_SIZES['chr22']}, BIN, 5000)\n",
    "df_correlations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.axes()\n",
    "# Show aggregated data, since we don't want error plots\n",
    "t = df_correlations.copy()\n",
    "t.loc[t['d'] == 0, 'd'] = BIN - 1\n",
    "t = df_correlations[['modification', 'd', 'correlation']].groupby(['modification', 'd']).mean().reset_index()\n",
    "g_results = sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\")\n",
    "sample_count = [1, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 50000]\n",
    "g_results.set(xscale='log')\n",
    "g_results.set(xticks=sample_count)\n",
    "g_results.set(xticklabels=sample_count)\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(BIN - 1, 50000)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.axes()\n",
    "sns.lineplot(data=t, x=\"d\", y=\"correlation\", hue=\"modification\")\n",
    "ax.title.set_text(f'Auto correlation bin={BIN}')\n",
    "ax.set_xlim(0, 1000)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
