{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on markup-extent\n",
    "\n",
    "“The authors developed the SPAN methodology which identifies peaks for ULI-ChIP-seq data using semi-supervised approach which uses user-annotations to train a model and choose parameters optimized for minimization of the training error. However, I could not find in the text the extent of manual annotations that should be done in order to achieve a reliable model. In addition, the test-error (manual vs. automated peaks annotation) is not reported. I thus suggest adding a table/plot summarizing the test error resulting from each extent of manual annotation.”\n",
    "\n",
    "Experiment design:\n",
    "\n",
    "* take ABF ChIP-seq data (5 targets, 40 donors)\n",
    "* investigate the effects of using k = 10, 20, 50, 100 (out of ~500) labels\n",
    "* for each k, create 10 random batches of k training labels and 500-k test labels \n",
    "  (only 5 batches for k=100 for obvious reasons)\n",
    "* for each batch, tune Span on training labels and report training and test error\n",
    "  average the errors across batches\n",
    "* in total, 7000 = 35 * 5 * 40 training-test error pairs were reported\n",
    "* from those, 800 = 4 * 5 * 40 mean training-test error pairs were generated\n",
    "\n",
    "Before launching this notebook, please launch `SpanTestErrorEvaluation` experiment from https://github.com/JetBrains-Research/epigenome"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import os;\n",
    "import matplotlib.pyplot as plt;\n",
    "import seaborn as sns;"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "TARGETS = [\"H3K27ac\", \"H3K27me3\", \"H3K4me3\", \"H3K4me1\", \"H3K36me3\"]\n",
    "TSVS = [os.path.join(\"/mnt/stripe/bio/experiments/span-test-error\", target + \".tsv\") for target in TARGETS]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataframes = [pd.read_csv(tsv, sep=\"\\t\", comment=\"#\") for tsv in TSVS]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for dataframe in dataframes:\n",
    "    track_id = [i // 35 for i in range(0, len(dataframe))]\n",
    "    dataframe[\"track_id\"] = track_id"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def aggregate_test_error(dataframe):\n",
    "    res = pd.DataFrame(columns=[\"k\", \"track_id\",\n",
    "                                \"mean_train_error\", \"mean_test_error\",\n",
    "                                \"sd_train_error\", \"sd_test_error\",\n",
    "                                \"median_train_error\", \"median_test_error\",\n",
    "                                \"median_macs2_error\", \"median_sicer_error\",\n",
    "                                \"mean_macs2_error\", \"mean_sicer_error\",\n",
    "                                \"median_span_error\", \"mean_span_error\"])\n",
    "    for k in sorted(set(dataframe[\"k\"])):\n",
    "        for track_id in sorted(set(dataframe[\"track_id\"])):\n",
    "            subset = dataframe[np.logical_and(dataframe[\"k\"] == k, dataframe[\"track_id\"] == track_id)]\n",
    "            mean_train_error = np.mean(subset[\"train_error\"])\n",
    "            sd_train_error = np.std(subset[\"train_error\"])\n",
    "            mean_test_error = np.mean(subset[\"test_error\"])\n",
    "            sd_test_error = np.std(subset[\"test_error\"])\n",
    "            median_train_error = np.median(subset[\"train_error\"])\n",
    "            median_test_error = np.median(subset[\"test_error\"])\n",
    "            median_macs2_error = np.median(subset[\"macs2_error\"])\n",
    "            mean_macs2_error = np.mean(subset[\"macs2_error\"])\n",
    "            median_sicer_error = np.median(subset[\"sicer_error\"])\n",
    "            mean_sicer_error = np.mean(subset[\"sicer_error\"])\n",
    "            median_span_error = np.median(subset[\"span_error\"])\n",
    "            mean_span_error = np.mean(subset[\"span_error\"])\n",
    "            res.loc[len(res)] = (k, track_id,\n",
    "                                 mean_train_error, mean_test_error,\n",
    "                                 sd_train_error, sd_test_error,\n",
    "                                 median_train_error, median_test_error,\n",
    "                                 median_macs2_error, median_sicer_error,\n",
    "                                 mean_macs2_error, mean_sicer_error,\n",
    "                                 median_span_error, mean_span_error)\n",
    "    return res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_error_dfs = [aggregate_test_error(dataframe) for dataframe in dataframes]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for target, test_error_df in zip(TARGETS, test_error_dfs):\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    for i, k in zip(range(len(ks)), ks):\n",
    "        data = test_error_df[test_error_df[\"k\"] == k][\"mean_train_error\"]\n",
    "        plt.boxplot(data, positions=[i], widths=0.6)\n",
    "    plt.xlim(-1, len(ks))    \n",
    "    plt.xticks(range(0,4), ks)\n",
    "    plt.title(target)\n",
    "    plt.xlabel('Training labels')\n",
    "    plt.ylabel('Mean training error')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}_train.png'.format(target), width=800, height=600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for target, test_error_df in zip(TARGETS, test_error_dfs):\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    for i, k in zip(range(len(ks)), ks):\n",
    "        data = test_error_df[test_error_df[\"k\"] == k][\"mean_test_error\"]        \n",
    "        plt.boxplot(data, positions=[i], widths=0.6)\n",
    "    plt.xlim(-1,4)    \n",
    "    plt.xticks(range(0,4), ks)\n",
    "    plt.title(target)\n",
    "    plt.xlabel('Training labels')\n",
    "    plt.ylabel('Mean test error')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}_test.png'.format(target), width=800, height=600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "COLOR_MAP = {10: 'k', 20: 'b', 50: 'y', 100: 'r'}\n",
    "for target, test_error_df in zip(TARGETS, test_error_dfs):\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    for k in ks:\n",
    "        data = test_error_df[test_error_df[\"k\"] == k]\n",
    "        plt.plot(data[\"mean_train_error\"], data[\"mean_test_error\"], 'o', color=COLOR_MAP[k])\n",
    "    plt.xlabel(\"Mean training error\")\n",
    "    plt.ylabel(\"Mean test error\")\n",
    "    plt.title(target)    \n",
    "    # plt.axis([0, 1, 0, 1])\n",
    "    plt.plot([0,1], [0,1], '-')\n",
    "    plt.legend(ks + [\"y=x\"])\n",
    "    plt.savefig('{}_train_test.png'.format(target), width=800, height=600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for target, test_error_df in zip(TARGETS, test_error_dfs):    \n",
    "    plot_dataframes = []\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    for i, k in zip(range(len(ks)), ks):\n",
    "        data_error = np.append(test_error_df[test_error_df[\"k\"] == k][\"median_train_error\"],\n",
    "                               test_error_df[test_error_df[\"k\"] == k][\"median_test_error\"])\n",
    "        data_k = [k] * len(data_error)\n",
    "        data_type = [\"train\"] * (len(data_error) // 2) + [\"test\"] * (len(data_error) // 2)\n",
    "        plot_dataframes.append(pd.DataFrame({\"k\": data_k, \"error\": data_error, \"type\": data_type}))    \n",
    "    sns.swarmplot(x=\"k\", y=\"error\", hue=\"type\", dodge=True, data=pd.concat(plot_dataframes))\n",
    "    plt.ylabel(\"median error\")\n",
    "    plt.title(target)\n",
    "    plt.legend(loc='upper right', title=None)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for target, test_error_df in zip(TARGETS, test_error_dfs):    \n",
    "    plot_dataframes = []\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    for i, k in zip(range(len(ks)), ks):\n",
    "        data_error = np.append(np.append(test_error_df[test_error_df[\"k\"] == k][\"median_test_error\"],\n",
    "                                         test_error_df[test_error_df[\"k\"] == k][\"median_macs2_error\"]),\n",
    "                               test_error_df[test_error_df[\"k\"] == k][\"median_sicer_error\"])\n",
    "        data_k = [k] * len(data_error)\n",
    "        data_type = [\"Span\"] * (len(data_error) // 3) + [\"MACS2\"] * (len(data_error) // 3) + [\"SICER\"] * (len(data_error) // 3)\n",
    "        plot_dataframes.append(pd.DataFrame({\"k\": data_k, \"error\": data_error, \"type\": data_type}))\n",
    "    sns.swarmplot(x=\"k\", y=\"error\", hue=\"type\", dodge=True, data=pd.concat(plot_dataframes))\n",
    "    plt.ylabel(\"Median test set error\")\n",
    "    plt.xlabel(\"Training labels\")\n",
    "    plt.title(target)\n",
    "    plt.legend(loc='upper right', title=None)\n",
    "    plt.savefig(f\"peak_callers_{target}.png\", width=800, height=600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for target, test_error_df in zip(TARGETS, test_error_dfs):    \n",
    "    plot_dataframes = []\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    for i, k in zip(range(len(ks)), ks):\n",
    "        data_error = np.append(np.append(test_error_df[test_error_df[\"k\"] == k][\"mean_test_error\"],\n",
    "                                         test_error_df[test_error_df[\"k\"] == k][\"mean_macs2_error\"]),\n",
    "                               test_error_df[test_error_df[\"k\"] == k][\"mean_sicer_error\"])\n",
    "        data_k = [k] * len(data_error)\n",
    "        data_type = [\"Span\"] * (len(data_error) // 3) + [\"MACS2\"] * (len(data_error) // 3) + [\"SICER\"] * (len(data_error) // 3)\n",
    "        plot_dataframes.append(pd.DataFrame({\"k\": data_k, \"error\": data_error, \"type\": data_type}))\n",
    "    sns.swarmplot(x=\"k\", y=\"error\", hue=\"type\", dodge=True, data=pd.concat(plot_dataframes))\n",
    "    plt.ylabel(\"Mean test set error\")\n",
    "    plt.xlabel(\"Training labels\")\n",
    "    plt.title(target)\n",
    "    plt.legend(loc='upper right', title=None)\n",
    "    plt.savefig(f\"peak_callers_{target}_mean.png\", width=800, height=600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for target, test_error_df in zip(TARGETS, test_error_dfs):\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    kmin = min(ks)\n",
    "    data_error = np.append(np.append(test_error_df[test_error_df[\"k\"] == kmin][\"median_macs2_error\"],\n",
    "                                     test_error_df[test_error_df[\"k\"] == kmin][\"median_sicer_error\"]),\n",
    "                           test_error_df[test_error_df[\"k\"] == kmin][\"median_span_error\"])\n",
    "    ndots = len(data_error) // 3\n",
    "    data_type = [\"MACS2\"] * ndots + [\"SICER\"] * ndots + [\"Span 0\"] * ndots\n",
    "    data_hue = [\"default\"] * len(data_error)\n",
    "    peak_callers = [\"MACS2\", \"SICER\", \"Span 0\"]\n",
    "    for i, k in zip(range(len(ks)), ks):\n",
    "        data_error = np.append(data_error, test_error_df[test_error_df[\"k\"] == k][\"median_test_error\"])\n",
    "        data_type = data_type + [f\"Span {k}\"] * ndots\n",
    "        data_hue = data_hue + [\"trained\"] * ndots\n",
    "        peak_callers.append(f\"Span {k}\")\n",
    "    plot_dataframe = pd.DataFrame({\"peak_caller\": data_type, \"error\": data_error, \"hue\": data_hue})\n",
    "    sns.swarmplot(x=\"peak_caller\", y=\"error\", hue=\"hue\", data=plot_dataframe)\n",
    "    medians = [np.median(plot_dataframe[plot_dataframe[\"peak_caller\"] == pc][\"error\"])\n",
    "               for pc in peak_callers]\n",
    "    for i, m in zip(range(0, len(medians)), medians):\n",
    "        plt.plot([i-0.4, i+0.4],[m, m], '-k', zorder=10)    \n",
    "    plt.ylabel(\"Median test set error\")\n",
    "    plt.xlabel(\"Peak caller\")\n",
    "    plt.title(target)\n",
    "    plt.legend(loc='upper right', title=None)    \n",
    "    plt.savefig(f\"peak_callers_{target}_median.png\", width=800, height=600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for target, test_error_df in zip(TARGETS, test_error_dfs):\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    kmin = min(ks)\n",
    "    data_error = np.append(np.append(test_error_df[test_error_df[\"k\"] == kmin][\"mean_macs2_error\"],\n",
    "                                     test_error_df[test_error_df[\"k\"] == kmin][\"mean_sicer_error\"]),\n",
    "                           test_error_df[test_error_df[\"k\"] == kmin][\"mean_span_error\"])\n",
    "    ndots = len(data_error) // 3\n",
    "    data_type = [\"MACS2\"] * ndots + [\"SICER\"] * ndots + [\"Span 0\"] * ndots\n",
    "    data_hue = [\"default\"] * len(data_error)\n",
    "    peak_callers = [\"MACS2\", \"SICER\", \"Span 0\"]\n",
    "    for i, k in zip(range(len(ks)), ks):\n",
    "        data_error = np.append(data_error, test_error_df[test_error_df[\"k\"] == k][\"mean_test_error\"])\n",
    "        data_type = data_type + [f\"Span {k}\"] * ndots\n",
    "        data_hue = data_hue + [\"trained\"] * ndots\n",
    "        peak_callers.append(f\"Span {k}\")\n",
    "    plot_dataframe = pd.DataFrame({\"peak_caller\": data_type, \"error\": data_error, \"hue\": data_hue})\n",
    "    sns.swarmplot(x=\"peak_caller\", y=\"error\", hue=\"hue\", data=plot_dataframe)\n",
    "    means = [np.mean(plot_dataframe[plot_dataframe[\"peak_caller\"] == pc][\"error\"])\n",
    "               for pc in peak_callers]\n",
    "    for i, m in zip(range(0, len(means)), means):\n",
    "        plt.plot([i-0.4, i+0.4],[m, m], '-k', zorder=10)    \n",
    "    plt.ylabel(\"Mean test set error\")\n",
    "    plt.xlabel(\"Peak caller\")\n",
    "    plt.title(target)\n",
    "    plt.legend(loc='upper right', title=None)\n",
    "    plt.plot([-0.5,6.5], [0,0], '-k')\n",
    "    plt.savefig(f\"peak_callers_{target}_mean.png\", width=800, height=600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "\n",
    "# Load files\n",
    "PATH = '/mnt/stripe/bio/experiments/span-test-error/tuned_span_peaks'\n",
    "KS = [10, 20, 50, 100]\n",
    "\n",
    "dfs = []\n",
    "for k in tqdm(KS):\n",
    "    files = glob.glob(os.path.join(PATH, 'k{}'.format(k)) + '/**/*.*')\n",
    "    fnames = [os.path.basename(f) for f in files]\n",
    "    ids = [fn.split('_')[1] for fn in fnames]\n",
    "    ks = [k] * len(files)\n",
    "    batches = [f.replace(os.path.join(PATH, 'k{}'.format(k)) + '/b', '').split('/')[0] for f in files]\n",
    "    modifications = [fn.split('_')[2] for fn in fnames]\n",
    "    dfs.append(pd.DataFrame({'k': ks, 'batch': batches, 'id': ids, 'file': files, 'modification': modifications}))\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "display(df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import downstream.bed_metrics as bm\n",
    "from pathlib import Path\n",
    "\n",
    "# Compute overlaps\n",
    "df_overlap = pd.DataFrame(columns=['id', 'modification', 'k', 'batch', 'overlap'])\n",
    "\n",
    "for k in KS:\n",
    "    for m in TARGETS:\n",
    "        for b in list(set(df['batch'])):\n",
    "            files = df.loc[\n",
    "                np.logical_and(df['k'] == k, np.logical_and(df['modification'] == m, df['batch'] == b))]['file']\n",
    "            paths = [Path(f) for f in files]\n",
    "            df_path = '/mnt/stripe/figures/overlap_{}_k{}_b{}.tsv'.format(m, k, b)\n",
    "            mt = bm.load_or_build_metrics_table(paths, paths, Path(df_path),\n",
    "                                                jaccard=False,\n",
    "                                                threads=30)\n",
    "            for r in mt.index:\n",
    "                for c in mt.columns:\n",
    "                    overlap = mt.loc[r][c]\n",
    "                    df_overlap.loc[len(df_overlap)] = (r + \"@\" + c, m, k, b, overlap)\n",
    "\n",
    "display(df_overlap.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_overlap_copy = df_overlap.copy()\n",
    "df_overlap.to_csv('/mnt/stripe/figures/overlap_batches.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Filter out failed tracks\n",
    "df_failed = pd.read_csv('/mnt/stripe/bio/experiments/configs/Y20O20/benchmark/Y20O20_peaks_summary_uli.tsv', \n",
    "                        sep='\\t', comment='#')\n",
    "df_failed = df_failed.loc[df_failed['status'] == 'failed'][['donor', 'modification']].drop_duplicates()\n",
    "# display(df_failed)\n",
    "failed = {}\n",
    "for m in TARGETS:\n",
    "    failed[m] = list(df_failed.loc[df_failed['modification'] == m]['donor'])\n",
    "display(failed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_overlap = pd.DataFrame(columns=df_overlap_copy.columns)\n",
    "filtered = 0\n",
    "for _, row in tqdm(df_overlap_copy.iterrows()):\n",
    "    rid, rm, _, _, _ = row\n",
    "    if all(d + '_' not in rid for d in failed[rm]):\n",
    "        df_overlap.loc[len(df_overlap)] = row\n",
    "    else:\n",
    "        filtered += 1\n",
    "        \n",
    "print('Filtered', filtered)\n",
    "print('Len', len(df_overlap))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_overlap.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_peaks = pd.read_csv('/mnt/stripe/bio/experiments/configs/Y20O20/benchmark/Y20O20_peaks_summary_uli.tsv', \n",
    "                        sep='\\t', comment='#')\n",
    "df_peaks = df_peaks.loc[df_peaks['status'] != 'failed']\n",
    "df_peaks.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Compute overlaps for SPAN not tuned\n",
    "df_span0_overlap = pd.DataFrame(columns=['id', 'modification', 'k', 'batch', 'overlap'])\n",
    "\n",
    "for m in TARGETS:\n",
    "    files = df_peaks.loc[\n",
    "        np.logical_and(df_peaks['modification'] == m, \n",
    "                       np.logical_and(df_peaks['tool'] == 'span', df_peaks['procedure'] != 'tuned'))]['file']\n",
    "    paths = [Path(f) for f in files]\n",
    "    df_path = '/mnt/stripe/figures/overlap_span_not_tuned_{}.tsv'.format(m)\n",
    "    mt = bm.load_or_build_metrics_table(paths, paths, Path(df_path),\n",
    "                                        jaccard=False,\n",
    "                                        threads=30)\n",
    "    for r in mt.index:\n",
    "        for c in mt.columns:\n",
    "            overlap = mt.loc[r][c]\n",
    "            df_span0_overlap.loc[len(df_span0_overlap)] = (r + \"@\" + c, m, 0, np.nan, overlap)\n",
    "\n",
    "display(df_span0_overlap.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Concat k=0 with experimental data\n",
    "df_overlap = pd.concat([df_span0_overlap, df_overlap])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_mean = df_overlap.groupby(['modification', 'k']).mean().reset_index()\n",
    "print('Mean overlap')\n",
    "display(df_mean)\n",
    "\n",
    "df_std = df_overlap.groupby(['modification', 'k']).std().reset_index()\n",
    "print('STD overlap')\n",
    "display(df_std)\n",
    "\n",
    "\n",
    "df_mean['mk'] = df_mean['modification'] + \" \" + list(map(str, df_mean['k']))\n",
    "mpl = len(set(df_mean['mk']))\n",
    "fig = plt.figure(figsize=(int(mpl * .75), 4))\n",
    "offset = 0\n",
    "for m in TARGETS:\n",
    "    datam = df_mean.loc[df_mean['modification'] == m]\n",
    "    datas = df_std.loc[df_std['modification'] == m]\n",
    "    xlabels = []\n",
    "    for t in datam['k']:\n",
    "        if t not in xlabels:\n",
    "            xlabels.append(t)\n",
    "    w = len(set(datam['mk']))\n",
    "    ax = plt.subplot2grid((1, mpl), (0, offset), colspan=w)\n",
    "    sns.lineplot(ax=ax, x='k', y='overlap', markers=True, data=datas)\n",
    "    ax.legend().set_visible(False)\n",
    "    ax.set_ylim(0, 1)\n",
    "    if offset > 0:\n",
    "        ax.get_yaxis().set_ticklabels([])\n",
    "        ax.set_ylabel('')\n",
    "    else:\n",
    "        ax.set_ylabel('std')\n",
    "    \n",
    "    ax2 = ax.twinx()    \n",
    "    sns.lineplot(ax=ax, x='k', y='overlap', markers=True, data=datam)\n",
    "    if m != TARGETS[-1]:\n",
    "        ax2.get_yaxis().set_ticklabels([])\n",
    "        ax2.set_ylabel('')\n",
    "    else:\n",
    "        ax2.set_ylabel('mean')\n",
    "   \n",
    "    offset += w\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_title(m)\n",
    "    plt.xticks(range(0, len(xlabels)), xlabels, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
