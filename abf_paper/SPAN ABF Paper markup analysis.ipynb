{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAN extent of required manual markup analysis\n",
    "\n",
    "Reviewer 1 says:\n",
    "“The authors developed the SCAN [sic!] methodology which identifies peaks for ULI-ChIP-seq data using semi-supervised approach which uses user-annotations to train a model and choose parameters optimized for minimization of the training error. However, I could not find in the text the extent of manual annotations that should be done in order to achieve a reliable model. In addition, the test-error (manual vs. automated peaks annotation) is not reported. I thus suggest adding a table/plot summarizing the test error resulting from each extent of manual annotation.”\n",
    "\n",
    "Experiment design: \n",
    "* take ABF ChIP-seq data (5 targets, 40 donors)\n",
    "* investigate the effects of using k = 10, 20, 50, 100 (out of ~500) labels\n",
    "* for each k, create 10 random batches of k training labels and 500-k test labels (only 5 batches for k=100 for obvious reasons)\n",
    "* for each batch, tune Span on training labels and report training and test error average the errors across batches\n",
    "* in total, 7000 = 35 * 5 * 40 training-test error pairs were reported\n",
    "* from those, 800 = 4 * 5 * 40 mean training-test error pairs were generated\n",
    "\n",
    "Slides: https://docs.google.com/presentation/d/1iRRPRe6XOqB85rsRHVJ8mNgLGcTV5EiXCapDh1a9g5w/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import os;\n",
    "import matplotlib.pyplot as plt;"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "TARGETS = [\"H3K27ac\", \"H3K27me3\", \"H3K4me3\", \"H3K4me1\", \"H3K36me3\"]\n",
    "TSVS = [os.path.join(\"/mnt/stripe/bio/experiments/span-test-error\", target + \".tsv\") for target in TARGETS]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataframes = [pd.read_csv(tsv, sep=\"\\t\", comment=\"#\") for tsv in TSVS]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for dataframe in dataframes:\n",
    "    track_id = [i // 35 for i in range(0, len(dataframe))]\n",
    "    dataframe[\"track_id\"] = track_id"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def aggregate_test_error(dataframe):\n",
    "    res = pd.DataFrame(columns=[\"k\", \"track_id\", \"mean_train_error\", \"mean_test_error\", \"sd_train_error\", \"sd_test_error\"])\n",
    "    for k in sorted(set(dataframe[\"k\"])):\n",
    "        for track_id in sorted(set(dataframe[\"track_id\"])):\n",
    "            subset = dataframe[np.logical_and(dataframe[\"k\"] == k, dataframe[\"track_id\"] == track_id)]\n",
    "            mean_train_error = np.mean(subset[\"train_error\"])\n",
    "            sd_train_error = np.std(subset[\"train_error\"])\n",
    "            mean_test_error = np.mean(subset[\"test_error\"])\n",
    "            sd_test_error = np.std(subset[\"test_error\"])\n",
    "            res.loc[len(res)] = (k, track_id, mean_train_error, mean_test_error, sd_train_error, sd_test_error)\n",
    "    return res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_error_dfs = [aggregate_test_error(dataframe) for dataframe in dataframes]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for target, test_error_df in zip(TARGETS, test_error_dfs):\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    for i, k in zip(range(len(ks)), ks):\n",
    "        data = test_error_df[test_error_df[\"k\"] == k][\"mean_train_error\"]\n",
    "        plt.boxplot(data, positions=[i], widths=0.6)\n",
    "    plt.xlim(-1, len(ks))    \n",
    "    plt.xticks(range(0,4), ks)\n",
    "    plt.title(target)\n",
    "    plt.xlabel('Training labels')\n",
    "    plt.ylabel('Mean training error')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"{}_train.png\".format(target), width=800, height=600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for target, test_error_df in zip(TARGETS, test_error_dfs):\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    for i, k in zip(range(len(ks)), ks):\n",
    "        data = test_error_df[test_error_df[\"k\"] == k][\"mean_test_error\"]        \n",
    "        plt.boxplot(data, positions=[i], widths=0.6)\n",
    "    plt.xlim(-1,4)    \n",
    "    plt.xticks(range(0,4), ks)\n",
    "    plt.title(target)\n",
    "    plt.xlabel('Training labels')\n",
    "    plt.ylabel('Mean test error')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"{}_test.png\".format(target), width=800, height=600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "COLOR_MAP = {10: 'k', 20: 'b', 50: 'y', 100: 'r'}\n",
    "for target, test_error_df in zip(TARGETS, test_error_dfs):\n",
    "    ks = sorted(set(int(k) for k in test_error_df[\"k\"]))\n",
    "    for k in ks:\n",
    "        data = test_error_df[test_error_df[\"k\"] == k]\n",
    "        plt.plot(data[\"mean_train_error\"], data[\"mean_test_error\"], 'o', color=COLOR_MAP[k])\n",
    "    plt.xlabel(\"Mean training error\")\n",
    "    plt.ylabel(\"Mean test error\")\n",
    "    plt.title(target)    \n",
    "    # plt.axis([0, 1, 0, 1])\n",
    "    plt.plot([0,1], [0,1], '-')\n",
    "    plt.legend(ks)\n",
    "    plt.savefig(\"{}_train_test.png\".format(target), width=800, height=600)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "mean_sd_errors = pd.DataFrame(columns=[\"target\", \"k\", \"mean_train_error\", \"sd_train_error\", \"mean_test_error\", \"sd_test_error\", \"test/train\", \"test - train\"])\n",
    "for target, dataframe in zip(TARGETS, dataframes):\n",
    "    ks = sorted(set(int(k) for k in dataframe[\"k\"]))\n",
    "    for k in ks:\n",
    "        subset = dataframe[dataframe[\"k\"] == k]\n",
    "        mean_train_error = np.mean(subset[\"train_error\"])\n",
    "        sd_train_error = np.std(subset[\"train_error\"])\n",
    "        mean_test_error = np.mean(subset[\"test_error\"])\n",
    "        sd_test_error = np.std(subset[\"test_error\"])\n",
    "        mean_sd_errors.loc[len(mean_sd_errors)] = (target, k, mean_train_error, sd_train_error, mean_test_error, sd_test_error, mean_test_error / mean_train_error, (mean_test_error - mean_train_error) * 100)\n",
    "display(mean_sd_errors)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
